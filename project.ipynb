{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c615c6",
   "metadata": {},
   "source": [
    "# Part A - Feature Extraction\n",
    "\n",
    "## 1. Feature Extraction\n",
    "\n",
    "This part of the coursework is involved with extracting all the features from the big folder of images celeba_selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ae406475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "EMBEDDINGS_FILE = \"extracted_features.npy\"\n",
    "NUM_IMAGES_TO_PROCESS = 20000\n",
    "IMAGE_DIR = \"celeba_selection\"\n",
    "ATTRIBUTE_FILE = \"list_attr_celeba.txt\"\n",
    "SAMPLE_ATTRIBUTES_FILENAME = \"celeba_sampled_attributes\"\n",
    "SELECTED_ATTRIBUTES = [\"Smiling\", \"Male\", \"Young\", \"Blond_Hair\", \"Wearing_Hat\"]\n",
    "PREDICTION_ATTRIBUTE = \"Smiling\"\n",
    "\n",
    "TRAINING_PERCENT = 0.7\n",
    "CALIB_PERCENT = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a82c862",
   "metadata": {},
   "source": [
    "First, we cut down the attribute dataset to only include the attributes we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a96043d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Smiling  Male  Young  Blond_Hair  Wearing_Hat\n",
      "000001.jpg        1    -1      1          -1           -1\n",
      "000002.jpg        1    -1      1          -1           -1\n",
      "000003.jpg       -1     1      1          -1           -1\n",
      "000004.jpg       -1    -1      1          -1           -1\n",
      "000005.jpg       -1    -1      1          -1           -1\n"
     ]
    }
   ],
   "source": [
    "# First we open the attribute file\n",
    "\n",
    "with open(ATTRIBUTE_FILE, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "\n",
    "# Extract the headers from first row\n",
    "\n",
    "columns = lines[1].strip().split()\n",
    "\n",
    "\n",
    "# Extract the data part (from line 2 onwards)\n",
    "\n",
    "dataRows = lines[2:]\n",
    "\n",
    "data = []\n",
    "imageNames = []\n",
    "for row in dataRows:\n",
    "    \n",
    "\n",
    "    # Seperating the image name (first column) from rest of the dataset\n",
    "    \n",
    "    parts = row.strip().split()\n",
    "    imageNames.append(parts[0])    \n",
    "    data.append([int(x) for x in parts[1:]])\n",
    "\n",
    "attributeData = pandas.DataFrame(data, columns=columns, index=imageNames)\n",
    "\n",
    "\n",
    "# Now we create the subset based on our specific attributes\n",
    "\n",
    "cutAttributes = attributeData[SELECTED_ATTRIBUTES]\n",
    "print(cutAttributes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb2ed2",
   "metadata": {},
   "source": [
    "The attribute dataset is now sampled randomly based on the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3ff577a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled attributes saved to: celeba_sampled_attributes.csv\n"
     ]
    }
   ],
   "source": [
    "sampledAttributes = cutAttributes.sample(n=NUM_IMAGES_TO_PROCESS, random_state=42)\n",
    "sampledImageNames = sampledAttributes.index.to_list()\n",
    "sampledAttributes.to_csv(SAMPLE_ATTRIBUTES_FILENAME + \".csv\", index=True)\n",
    "\n",
    "print(f\"Sampled attributes saved to: {SAMPLE_ATTRIBUTES_FILENAME}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef397e3",
   "metadata": {},
   "source": [
    "Below is the code for executing the feature extraction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6e67208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures():\n",
    "\n",
    "    # First we skip this expensive step if feature file already exists.\n",
    "\n",
    "    if os.path.exists(EMBEDDINGS_FILE):\n",
    "        print(f\"Embeddings file '{EMBEDDINGS_FILE} already exists. Extracting is skipped\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    # We now load the pre-trained Vision Transformer and set it to evaluation mode\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    vit = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1).to(device)\n",
    "    vit.eval()\n",
    "\n",
    "\n",
    "    # Preprocessing is defined; resizing, centre-cropping to 224x224, converted to tensors, and normalised to match ImageNet training\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "    # List of images is prepared from feature list earlier\n",
    "\n",
    "    sampledAttributes = pandas.read_csv(SAMPLE_ATTRIBUTES_FILENAME + \".csv\", index_col=0)\n",
    "    sampledImageNames = sampledAttributes.index.tolist()\n",
    "    \n",
    "\n",
    "\n",
    "    allFeatures = []\n",
    "\n",
    "\n",
    "    # Features are extracted for each image\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for fname in tqdm(sampledImageNames, desc=\"Extracting ViT Feature Embeddings\", colour=\"#ebbcba\"):\n",
    "            img = Image.open(os.path.join(IMAGE_DIR, fname)).convert(\"RGB\")\n",
    "            x = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "            ## Forward pass to get feature embeddings without classification head\n",
    "\n",
    "            x_processed = vit._process_input(x)\n",
    "            n = x_processed.shape[0]\n",
    "            \n",
    "            batch_class_token = vit.class_token.expand(n, -1, -1)\n",
    "            x_with_token = torch.cat([batch_class_token, x_processed], dim=1)\n",
    "\n",
    "            encoded_features = vit.encoder(x_with_token)\n",
    "            features = encoded_features[:, 0]\n",
    "\n",
    "            allFeatures.append(features.cpu().numpy().flatten())\n",
    "    \n",
    "\n",
    "    all_features_np = numpy.array(allFeatures)\n",
    "    print(f\"Feature matrix shape: {all_features_np.shape}\")\n",
    "\n",
    "\n",
    "    # Save feature embeddings for future use\n",
    "\n",
    "    numpy.save(EMBEDDINGS_FILE, all_features_np)\n",
    "    print(f\"Embeddings saved to '{EMBEDDINGS_FILE}'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1688c6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76e9aecd40e45518ff000175626e19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting ViT Feature Embeddings:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'celeba_selection\\\\093242.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mextractFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mextractFeatures\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m tqdm(sampledImageNames, desc=\u001b[33m\"\u001b[39m\u001b[33mExtracting ViT Feature Embeddings\u001b[39m\u001b[33m\"\u001b[39m, colour=\u001b[33m\"\u001b[39m\u001b[33m#ebbcba\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m         img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMAGE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m         x = preprocess(img).unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m     45\u001b[39m         \u001b[38;5;66;03m## Forward pass to get feature embeddings without classification head\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rshie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\PIL\\Image.py:3513\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3512\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3513\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3514\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'celeba_selection\\\\093242.jpg'"
     ]
    }
   ],
   "source": [
    "extractFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e51e3",
   "metadata": {},
   "source": [
    "## 2. Training the Classifiers\n",
    "\n",
    "First, we need top load the features and labels generated in part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = numpy.load(EMBEDDINGS_FILE)\n",
    "sampledAttributes = pandas.read_csv(SAMPLE_ATTRIBUTES_FILENAME + \".csv\", index_col=0)\n",
    "\n",
    "\n",
    "\n",
    "# Only want to confirm if the image is similing or not\n",
    "\n",
    "labels = sampledAttributes[PREDICTION_ATTRIBUTE].values\n",
    "\n",
    "# Since the attribute data records binary values as -1 and 1, we need to convert to 0 and 1 for Gaussian/Normal classifier\n",
    "\n",
    "labels = ((labels + 1) // 2).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2567e277",
   "metadata": {},
   "source": [
    "Now, the split sizes need to be computed to ensure the correct number of cases for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21cfdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSize = int(TRAINING_PERCENT * NUM_IMAGES_TO_PROCESS)\n",
    "calibSize = int(CALIB_PERCENT * NUM_IMAGES_TO_PROCESS)\n",
    "testSize = NUM_IMAGES_TO_PROCESS - trainSize - calibSize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d0a8b1",
   "metadata": {},
   "source": [
    "The datasets can now be produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27ef12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "\n",
    "XTrain = features[:trainSize]\n",
    "yTrain = labels[:trainSize]\n",
    "\n",
    "# Calibration Set\n",
    "\n",
    "XCalib = features[trainSize:trainSize + calibSize]\n",
    "yCalib = labels[trainSize:trainSize + calibSize]\n",
    "\n",
    "# Testing Set\n",
    "\n",
    "XTest = features[trainSize + calibSize:]\n",
    "yTest = labels[trainSize + calibSize:]\n",
    "\n",
    "print(\"Train:\", XTrain.shape, yTrain.shape)\n",
    "print(\"Calibration:\", XCalib.shape, yCalib.shape)\n",
    "print(\"Test:\", XTest.shape, yTest.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
